---
title: "kaggle"
author: "Arnab Aich"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  chunk_output_type: inline
---
* Loading required Libraries 
```{r message=FALSE, warning=FALSE,echo=TRUE}
packages = c("parallel","doParallel","doSNOW","readr","dplyr","caret","pROC","ggplot2","gridExtra","mice","xgboost","gbm","e1071","keras","tensorflow","reticulate")
invisible(xfun::pkg_attach(packages))
```

# data
```{r message=FALSE, warning=FALSE, include=FALSE}
train_data <- read_csv("training_data.csv",show_col_types = FALSE)
dim(train_data)
train_Y = train_data$SalePrice
train_X <- train_data[,-which(colnames(train_data)=="SalePrice")]
n_train = dim(train_X)[1]
test_X <- read_csv("test_data.csv",show_col_types = FALSE);dim(test_X)

X = rbind(train_X,test_X);dim(X)
p_NA = 0.5              # max allowable proportion of NA values
my.cluster =  makeCluster(detectCores()-4)
pNA= parApply(my.cluster,X,2,function(x){mean(is.na(x))}) # Proportion of NA values
stopCluster(my.cluster)
# selecting variables containing less NA values than threshold
X <- data.frame(X[,-c(as.numeric(which(pNA<p_NA)))]);dim(X)

# Missing value imputation
X_miss = mice(X,printFlag = FALSE,maxit = 10,m=10,method = "norm")
X1 = complete(X_miss)

# Numeric variables
num_X<- X1 %>% select_if(is.numeric);dim(num_X)
P = apply(num_X,2,function(x){sum(x,na.rm = TRUE)})
# checking for variables containing zeroes only
t = as.numeric(which(P==0));length(t)  
num1 <- data.frame(num_X[,-c(t)]);dim(num1)
# removing variables with NA
t2 = as.numeric(which(is.na(colMeans(num1))))
num = data.frame(num1[,-c(t2)]);dim(num)

X_train_num = num[1:n_train,];dim(X_train_num)
X_test_num = num[- c(1:n_train),];dim(X_test_num)

x_mean=as.numeric(colMeans(X_train_num ,na.rm = TRUE));length(x_mean)
x_sd =as.numeric(apply(X_train_num,2,sd,na.rm = TRUE))


# Character data
cat_X <- X1 %>% select_if(~ !is.numeric(.));dim(cat_X)
cat = data.frame(apply(cat_X,2,function(y){as.numeric(factor(y,exclude = NULL))}));dim(cat)
X_train_cat = cat[1:n_train,]
X_test_cat = cat[- seq(1,n_train),]


X_train = cbind(scale(X_train_num,center=x_mean,scale=x_sd),X_train_cat);dim(X_train)
X_test = cbind(scale(X_test_num,center=x_mean,scale=x_sd),X_test_cat);dim(X_test)
data_train = data.frame(y = train_Y,x = X_train)
```
# Extreme Gradiant Boosting
```{r message=FALSE, warning=FALSE}
dmatrix = xgb.DMatrix(data = as.matrix(X_train),label = train_Y)
model1 = xgboost(data = dmatrix,nrounds = 200,eta = 1,max_depth = 25)
fitted_saleprice1=predict(model1,as.matrix(X_test))
```

# Support Vector Regression
```{r echo=FALSE, message=FALSE, warning=FALSE}
model2 = svm(y~.,data = data_train)
fitted_saleprice2=predict(model2,as.matrix(X_test))
```

# Neural Network
```{r}
num_cores <- detectCores() - 2 
# Build and compile your model as before
model3 <- keras_model_sequential()

model3 %>%
  layer_dense(units = ncol(X_train), activation = 'linear', input_shape = ncol(X_train)) %>%
  layer_dense(units = 40, activation = 'relu') %>%
  layer_dense(units = 20, activation = 'tanh') %>%
  # layer_dense(units = floor(ncol(X_train)*0.25), activation = 'relu') %>%
  # layer_dense(units = 5, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'linear')

summary(model3)
model3 %>% compile(
  loss = "mean_squared_error",
   optimizer = optimizer_adam(learning_rate = learning_rate_schedule_exponential_decay(initial_learning_rate = 3, decay_steps = 100,decay_rate =0.05))
)


# Train the model with parallel processing
history <- model3 %>% fit(
  as.matrix(X_train), train_Y,
  epochs = 100,
  batch_size = ncol(X_train)*0.05
  # validation_data = list(x_test, y_test),
    , workers = num_cores # Number of workers for data loading
  , use_multiprocessing = TRUE  # Enable multiprocessing
)

fitted_saleprice3=predict(model3,as.matrix(X_test))
```
```{r}
cbind(id = test_X$RecordID,estimate_1 =fitted_saleprice1,estimate_2 =fitted_saleprice2,estimate_3=fitted_saleprice3)
```





